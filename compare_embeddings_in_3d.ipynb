{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "from falcon.falcon_util import *\n",
    "from falcon.model_editor import *\n",
    "from falcon.find_with_wiki import *\n",
    "\n",
    "import torch, random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_name = 'ROME'\n",
    "model_name = 'gpt2-xl'\n",
    "# model_name = 'meta-llama/Meta-Llama-3-8B'\n",
    "hparams_fname = f'{model_name}.json'\n",
    "ds_name = 'cf'\n",
    "num_edits = 1\n",
    "\n",
    "dataset_size_limit = None\n",
    "continue_from_run = None\n",
    "skip_generation_tests = False\n",
    "generation_test_interval = 1\n",
    "conserve_memory = False\n",
    "dir_name = alg_name\n",
    "use_cache = False\n",
    "output_hidden_states = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_editor = ModelEditor(\n",
    "    alg_name, model_name, hparams_fname, ds_name,\n",
    "    dataset_size_limit, continue_from_run, skip_generation_tests,\n",
    "    generation_test_interval, conserve_memory, dir_name, num_edits, use_cache, output_hidden_states\n",
    ")\n",
    "\n",
    "# 원본 모델\n",
    "model, tok = deepcopy(model_editor._model), model_editor._tok\n",
    "\n",
    "# 편집된 모델\n",
    "# model_editor.load_data()\n",
    "# model_editor.edit()\n",
    "# edited_model = model_editor._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(in_file_path, select_ids: set, do_edit):\n",
    "    print(f'load_data() in_file_path : {in_file_path}')\n",
    "    in_file = open_file(in_file_path, mode='r')\n",
    "    datas = json.load(in_file)\n",
    "    print(f'load_data() datas size : {len(datas)}')\n",
    "\n",
    "    data_dict = {}\n",
    "    for data in datas:\n",
    "        case_id = data['case_id']\n",
    "\n",
    "        if not do_edit:\n",
    "            target_id = data['requested_rewrite']['target_true']['id']\n",
    "            target = data['requested_rewrite']['target_true']['str']\n",
    "        else:\n",
    "            target_id = data['requested_rewrite']['target_new']['id']\n",
    "            target = data['requested_rewrite']['target_new']['str']\n",
    "        \n",
    "        if not target_id in select_ids:\n",
    "            continue\n",
    "\n",
    "        key = f'{target_id}\\t{target}'\n",
    "\n",
    "        if not key in data_dict.keys():\n",
    "            data_dict[key] = []\n",
    "        data_dict[key].append(data)\n",
    "    \n",
    "    key_size, value_size = len(data_dict), sum(len(v) for v in data_dict.values())\n",
    "    print(f'load_data() data_dict key size : {key_size}')\n",
    "    print(f'load_data() data_dict value size : {value_size}\\n')\n",
    "\n",
    "    return datas, data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(in_file_path1, in_file_path2, out_file_path):\n",
    "    in_file1 = open_file(in_file_path1, mode='r')\n",
    "    datas1 = json.load(in_file1)\n",
    "    print(f'merge_data() datas1 len : {len(datas1)}, in_file_path1 : {in_file_path1}')\n",
    "\n",
    "    idx_max = -1\n",
    "    for data in datas1:\n",
    "        case_id = data['case_id']\n",
    "        if idx_max < case_id:\n",
    "            idx_max = case_id\n",
    "\n",
    "    in_file2 = open_file(in_file_path2, mode='r')\n",
    "    datas2 = json.load(in_file2)\n",
    "    print(f'merge_data() datas2 len : {len(datas2)}, in_file_path2 : {in_file_path2}')\n",
    "\n",
    "    for data in datas2:\n",
    "        idx_max += 1\n",
    "        data['case_id'] = idx_max\n",
    "        datas1.append(data)\n",
    "    \n",
    "    print(f'merge_data() datas_merged len : {len(datas1)} -> out_file_path : {out_file_path}\\n')\n",
    "    write_json_to_file(datas1, out_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tok, text: str, do_print=False):\n",
    "    text_encs = tok(text, return_tensors='pt')['input_ids'][0]\n",
    "\n",
    "    if do_print:\n",
    "        for text_enc in text_encs:\n",
    "            print(f'{text_enc}\\t\\t->\\t{tok.decode(text_enc)}')\n",
    "        print()\n",
    "    \n",
    "    return text_encs, len(text_encs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dim(data_org, method, n_components, random_state):\n",
    "    data = np.copy(data_org)\n",
    "\n",
    "    (N, L, D) = data.shape\n",
    "\n",
    "    # (N * L, D) 형태로 데이터 reshape\n",
    "    reshaped_data = data.reshape(-1, D)  # (N * L, D)\n",
    "    # print(reshaped_data.shape)\n",
    "\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)    \n",
    "    elif method == 'tsne':\n",
    "        reducer = TSNE(n_components=n_components, random_state=random_state)\n",
    "    elif method == 'umap':\n",
    "        reducer = umap.UMAP(n_components=n_components, random_state=random_state)\n",
    "\n",
    "    reduced_data = reducer.fit_transform(reshaped_data)  # (N * L, 3)\n",
    "    reduced_data = reduced_data.reshape(N, L, n_components)\n",
    "    return reduced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_from_reduce(words, data, title, method, save_path: str, do_fix):\n",
    "    (N, L, D) = data.shape\n",
    "    \n",
    "    # 레이어별 색상 정의\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, L))  # L개의 색상 생성\n",
    "    colors[-1] = np.array([1, 0, 0, 1])\n",
    "\n",
    "    # 시각화 (3D Scatter Plot)\n",
    "    fig = plt.figure(figsize=(20, 14))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # 각 레이어별로 데이터 시각화\n",
    "    for layer in range(L):\n",
    "        layer_data = data[:, layer, :]  # 해당 레이어의 모든 데이터 (N, 3)\n",
    "\n",
    "        # 전체 데이터 포인트 동일 색상\n",
    "        ax.scatter(layer_data[:, 0], layer_data[:, 1], layer_data[:, 2], color=colors[layer], label=f'Layer {layer+1}', s=100, alpha=0.7)\n",
    "        \n",
    "        # 마지막 데이터 포인트만 다른 색상으로\n",
    "        # ax.scatter(layer_data[:-1, 0], layer_data[:-1, 1], layer_data[:-1, 2], color=colors[layer], label=f'Layer {layer+1}', s=100, alpha=0.7)\n",
    "        # ax.scatter(layer_data[-1, 0], layer_data[-1, 1], layer_data[-1, 2], color='red', s=150, alpha=0.9, edgecolor='k', label=f'Last Point (Layer {layer+1})')\n",
    "\n",
    "        # 데이터 포인트에 레이블(words) 추가\n",
    "        for i in range(N):\n",
    "            if i == N-1:\n",
    "                # word = f'({i})_{words[i]}' # 각 데이터에 해당하는 레이블\n",
    "                word = i\n",
    "                color, fontsize, weight = 'red', 12, 'bold' # 마지막 데이터 포인트만 다르게\n",
    "            else:\n",
    "                word = i\n",
    "                color, fontsize, weight = 'black', 10, 'normal' # 마지막 데이터 포인트만 다르게\n",
    "            \n",
    "            # word = i\n",
    "            ax.text(layer_data[i, 0], layer_data[i, 1], layer_data[i, 2], word, ha='center', va='center', alpha=0.8,\n",
    "                    color=color, fontsize=fontsize, weight=weight)\n",
    "        \n",
    "    def _set1():\n",
    "        # 범위 고정\n",
    "        if do_fix:\n",
    "            # if not 'tsne' in title:\n",
    "            #     size_ = 50\n",
    "            # else:\n",
    "            #     size_ = 200\n",
    "            \n",
    "            # ax.set_xlim([-1*size_, size_])\n",
    "            # ax.set_ylim([-1*size_, size_])\n",
    "            # ax.set_zlim([-1*size_, size_])\n",
    "\n",
    "            # 모든 축에 대해 개별적으로 최대값과 최소값 설정\n",
    "            all_data = data.reshape(-1, D)  # (N * L, 3)로 변환\n",
    "            x_min, y_min, z_min = all_data.min(axis=0) - 10  # 여백 추가\n",
    "            x_max, y_max, z_max = all_data.max(axis=0) + 10  # 여백 추가\n",
    "\n",
    "            # 각 축의 범위 설정\n",
    "            ax.set_xlim([x_min, x_max])\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "            ax.set_zlim([z_min, z_max])\n",
    "\n",
    "            ax.view_init(elev=30, azim=30)  # 고정된 각도에서 시각화 (elev는 고도, azim은 방위각)\n",
    "        \n",
    "        # 범위 자동 설정: 전체 데이터의 최소/최대값 계산\n",
    "        else:\n",
    "            all_data = data.reshape(-1, D)  # (N * L, 3)로 변환\n",
    "            x_min, y_min, z_min = all_data.min(axis=0) - 5  # 여백 추가\n",
    "            x_max, y_max, z_max = all_data.max(axis=0) + 5  # 여백 추가\n",
    "\n",
    "            # 축 범위 설정\n",
    "            ax.set_xlim([x_min, x_max])\n",
    "            ax.set_ylim([y_min, y_max])\n",
    "            ax.set_zlim([z_min, z_max])\n",
    "\n",
    "    _set1()\n",
    "\n",
    "\n",
    "    # 그래프 꾸미기\n",
    "    vi_title = f'[ {title} ] with {method}'\n",
    "    ax.set_title(f'3D Visualization of Data across Layers {vi_title}')\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    ax.set_zlabel('Component 3')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if not do_fix:\n",
    "        save_path = save_path.format(title, 'auto', title, method)\n",
    "    else:\n",
    "        save_path = save_path.format(title, 'fix', title, method)\n",
    "\n",
    "    make_parent(save_path)\n",
    "    fig.savefig(save_path, dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings_3d_by_layer(words, embeddings, title, n_components=3, save_path='', random_state=42):\n",
    "    methods = ['pca', 'tsne', 'umap']\n",
    "    # methods = ['pca']\n",
    "\n",
    "    for method in methods:\n",
    "        embeddings_reduce = reduce_dim(embeddings, method, n_components, random_state)\n",
    "        plot_from_reduce(words, embeddings_reduce, title, method, save_path, do_fix=False)\n",
    "        plot_from_reduce(words, embeddings_reduce, title, method, save_path, do_fix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_token(batch_hidden_states, do_print):\n",
    "    # 하나의 엔티티가 여러 토큰으로 쪼개진 경우 -> 모든 토큰 벡터의 평균을 해당 엔티티의 벡터로 사용\n",
    "    ent_states = []\n",
    "\n",
    "    for hidden_states in batch_hidden_states:\n",
    "        ent_state = hidden_states.mean(dim=0)\n",
    "        ent_states.append(ent_state.detach().cpu().numpy())\n",
    "    \n",
    "    ent_states = np.array(ent_states)\n",
    "    \n",
    "    if do_print:\n",
    "        shape_org = (len(batch_hidden_states),) + batch_hidden_states[0].shape\n",
    "        shape = ent_states.shape\n",
    "\n",
    "        print(f'average_token() batch_hidden_states shape : (N, T, L, D) = {shape_org}')\n",
    "        print(f'average_token() ent_states shape : (N, L, D) = {shape}\\n')\n",
    "    \n",
    "    return ent_states\n",
    "\n",
    "\n",
    "def compare_embedding_in_3d(model: AutoModelForCausalLM, tok: AutoTokenizer, data_dict: dict, save_path: str, do_print=False):\n",
    "    for key in data_dict.keys():\n",
    "        print(f'target_id : {key}\\n')\n",
    "\n",
    "        batch_hidden_states = []\n",
    "        prompt_for_gens = []\n",
    "        target = ''\n",
    "\n",
    "        ''' 데이터 하나씩 확인 '''\n",
    "        for data in data_dict[key]:\n",
    "            case_id = data['case_id']\n",
    "            prompt = data['requested_rewrite']['prompt']\n",
    "            relation_id = data['requested_rewrite']['relation_id']\n",
    "            target = ' ' + data['requested_rewrite']['target_true']['str']\n",
    "            target_id = data['requested_rewrite']['target_true']['id']\n",
    "            subject = data['requested_rewrite']['subject']\n",
    "            prompt_for_gen = prompt.format(subject)\n",
    "\n",
    "            prompt_tok_ids, prompt_tok_len = tokenize(tok, prompt_for_gen, do_print)\n",
    "            target_tok_ids, target_tok_len = tokenize(tok, target, do_print)\n",
    "\n",
    "            if 'gpt' in model_name:\n",
    "                target_tok_len += 1\n",
    "\n",
    "            hidden_states = None\n",
    "            for i in range(1, target_tok_len):\n",
    "                text_gen, outputs_1 = generate(model, tok, [prompt_for_gen], 1, prompt_tok_len+i)\n",
    "                prompt_for_gen = text_gen\n",
    "\n",
    "                outputs_2 = model(**tok([prompt_for_gen], return_tensors='pt').to('cuda'))\n",
    "                hidden_states = outputs_2.hidden_states\n",
    "\n",
    "                if do_print:\n",
    "                    print(f'text_gen : {text_gen}')\n",
    "                    print(f'outputs : {outputs_1}\\n')\n",
    "\n",
    "                    for layer_idx, layer_hidden_state in enumerate(hidden_states):\n",
    "                        print(f'Layer {layer_idx} shape: {layer_hidden_state.shape}')\n",
    "                    print()\n",
    "            \n",
    "            '''\n",
    "                원래 hidden_states 는 ['레이어 별로' / '토큰 별로' / '벡터']\n",
    "                이걸 ['토큰 별로' / '레이어 별로' / '벡터']로 변환\n",
    "            '''\n",
    "            _hidden_states = []\n",
    "            for layer_idx, layer_hidden_state in enumerate(hidden_states):            \n",
    "                _hidden_states.append(layer_hidden_state)\n",
    "            \n",
    "            stacked_hidden_states = torch.cat(_hidden_states, dim=0)\n",
    "            transposed_hidden_states = stacked_hidden_states.permute(1, 0, 2)\n",
    "            # print(transposed_hidden_states.shape)\n",
    "\n",
    "            target_hidden_states = transposed_hidden_states[(-1*(target_tok_len-1)):] # 전체 토큰이 아닌 target 부분만 가져옴\n",
    "            # print(target_hidden_states.shape)\n",
    "\n",
    "            batch_hidden_states.append(target_hidden_states)\n",
    "            prompt_for_gens.append(prompt.format(subject))\n",
    "        \n",
    "\n",
    "        ent_states = average_token(batch_hidden_states, do_print)\n",
    "        (N, L, D) = ent_states.shape\n",
    "\n",
    "        # ent_states = ent_states.reshape(N, 7, 7, D)\n",
    "        # ent_states = ent_states.mean(axis=2)\n",
    "        # print(ent_states.shape)\n",
    "\n",
    "        plot_embeddings_3d_by_layer(prompt_for_gens, ent_states, target.strip(), 3, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(data_dir, ds_name, model_name, select_ids: set, do_print=False):\n",
    "    in_path = f'{data_dir}/find_with_wiki_from_{ds_name}_{model_name}'\n",
    "    print(f'in_path : {in_path}')\n",
    "\n",
    "    in_file_path_org = f'{in_path}/json/find_knowledge_in_model_with_wiki.json'\n",
    "    in_file_path_edit = f'{in_path}/json/find_knowledge_in_model_with_wiki_edit.json'\n",
    "    in_file_path_new = f'{in_path}/json/find_knowledge_in_model_with_wiki_new.json'\n",
    "    in_file_path_merged = f'{in_path}/json/find_knowledge_in_model_with_wiki_merged.json'\n",
    "\n",
    "    # 데이터 로드\n",
    "    datas_org, data_dict_org = load_data(in_file_path_org, select_ids, False)\n",
    "    datas_edit, data_dict_edit = load_data(in_file_path_edit, select_ids, True)\n",
    "    merge_data(in_file_path_org, in_file_path_new, in_file_path_merged)\n",
    "    datas_merged, data_dict_merged = load_data(in_file_path_merged, select_ids, False)\n",
    "\n",
    "    \n",
    "    # 1. 원본 모델, '내재된 지식' 데이터\n",
    "    save_path = f'{in_path}/images/' + '{}/lim_{}/{}_{}_1_org_org.jpg'\n",
    "    compare_embedding_in_3d(model, tok, data_dict_org, save_path, do_print)\n",
    "\n",
    "    # key 별로 편집하고, 시각화\n",
    "    for data_key in data_dict_org.keys():\n",
    "        # 모델 편집\n",
    "        _datas = data_dict_edit[data_key]\n",
    "        model_editor.edit_ext_datas(_datas)\n",
    "        edited_model = model_editor._model\n",
    "\n",
    "        # 2. 편집 모델, '내재된 지식' 데이터\n",
    "        _data_dict = {data_key: data_dict_org[data_key]}\n",
    "        save_path = f'{in_path}/images/' + '{}/lim_{}/{}_{}_2_edit_org.jpg'\n",
    "        compare_embedding_in_3d(edited_model, tok, _data_dict, save_path, do_print)\n",
    "\n",
    "        # 3. 편집 모델, '내재된 지식 + 편집' 데이터\n",
    "        _data_dict = {data_key: data_dict_merged[data_key]}\n",
    "        save_path = f'{in_path}/images/' + '{}/lim_{}/{}_{}_3_edit_eidt.jpg'\n",
    "        compare_embedding_in_3d(edited_model, tok, _data_dict, save_path, do_print)\n",
    "\n",
    "        # 편집된 가중치 복원\n",
    "        model_editor.restore_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'./data'\n",
    "ds_name = 'mquake'\n",
    "model_name = 'gpt'\n",
    "\n",
    "select_ids = ['Q61', 'Q62', 'Q49', 'Q95', 'Q2283']\n",
    "run(data_dir, ds_name, model_name, set(select_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = []\n",
    "\n",
    "# d1 = torch.randn(1, 2, 3)\n",
    "# print(f'{d1.shape}\\n{d1}\\n')\n",
    "\n",
    "# d2 = torch.randn(1, 2, 3)\n",
    "# print(f'{d2.shape}\\n{d2}\\n')\n",
    "\n",
    "# ds.append(d1)\n",
    "# ds.append(d2)\n",
    "\n",
    "# stacked_ds = torch.cat(ds, dim=0)\n",
    "# print(f'{stacked_ds.shape}\\n{stacked_ds}\\n')\n",
    "\n",
    "# transposed_ds = stacked_ds.permute(1, 0, 2)\n",
    "# print(f'{transposed_ds.shape}\\n{transposed_ds}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "memit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
